app:
  head:
    title: 'ReVP: Reversible Visual Processing with Latent Models'
    meta:
    - name: 'description'
      content: "As large pretrained models gain traction, we find that large pretrained autoencoders (AEs) are surprisingly suitable for image steganography. Unlike previous methods, our model performs message reconstruction in the latent space of an AE, enabling effortless training of a pair of steganographic encoder-decoder using a simple L2 loss. With a small number of additional tunable parameters (around 10% of the base AE), our method reconstructs intricate high-frequency details. To further examine our method in practical scenarios, we test it in two novel applications: reversible visual censorship and reversible AI editing. Additionally, we provide modules to enhance our model's robustness to cropping, quantization, and compression. Our method is straightforward and easy to implement. Despite its simplicity, it works effectively on both in-the-wild images and standard datasets across various applications, offering competitive perceptual quality compared to specialized models."
    - name: "google-site-verification"
      content: "I9VHWgYS9g3o8RUJ7ryB1hPoIcN9YK1bX1Ds7_uDIYE"
  meta:
    title: 'ReVP: Reversible Visual Processing with Latent Models'
    ogTitle: 'ReVP: Reversible Visual Processing with Latent Models'
    description: "As large pretrained models gain traction, we find that large pretrained autoencoders (AEs) are surprisingly suitable for image steganography. Unlike previous methods, our model performs message reconstruction in the latent space of an AE, enabling effortless training of a pair of steganographic encoder-decoder using a simple L2 loss. With a small number of additional tunable parameters (around 10% of the base AE), our method reconstructs intricate high-frequency details. To further examine our method in practical scenarios, we test it in two novel applications: reversible visual censorship and reversible AI editing. Additionally, we provide modules to enhance our model's robustness to cropping, quantization, and compression. Our method is straightforward and easy to implement. Despite its simplicity, it works effectively on both in-the-wild images and standard datasets across various applications, offering competitive perceptual quality compared to specialized models."
    ogDescription: "As large pretrained models gain traction, we find that large pretrained autoencoders (AEs) are surprisingly suitable for image steganography. Unlike previous methods, our model performs message reconstruction in the latent space of an AE, enabling effortless training of a pair of steganographic encoder-decoder using a simple L2 loss. With a small number of additional tunable parameters (around 10% of the base AE), our method reconstructs intricate high-frequency details. To further examine our method in practical scenarios, we test it in two novel applications: reversible visual censorship and reversible AI editing. Additionally, we provide modules to enhance our model's robustness to cropping, quantization, and compression. Our method is straightforward and easy to implement. Despite its simplicity, it works effectively on both in-the-wild images and standard datasets across various applications, offering competitive perceptual quality compared to specialized models."
    ogImage: 'https://revp2024.github.io/images/teaser.jpg'
    twitterCard: 'summary_large_image'

header:
  title: 'ReVP<div>Reversible Visual Processing with Latent Models</div>'
  venue: '2024'
  authors:
  - text: 'Anonymous'
    homepage: ''
    mark: ''

  affiliations:
  - text: ''
    mark: ''

  links:
  - text: 'Video'
    url: 'https://youtu.be/v0lHsiwnxh0'
    icon:
    - 'fa-brands'
    - 'fa-youtube'
  - text: 'Demo'
    url: 'https://huggingface.co/spaces/revp2024/revp'
    icon:
    - 'fa-regular'
    - 'fa-face-smiling-hands'

