<!DOCTYPE html>
<html >
<head><meta charset="utf-8">
<title>ReVP: Reversible Visual Processing with Latent Models</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="google-site-verification" content="I9VHWgYS9g3o8RUJ7ryB1hPoIcN9YK1bX1Ds7_uDIYE">
<meta property="og:title" content="ReVP: Reversible Visual Processing with Latent Models">
<meta name="description" content="As large pretrained models gain traction, we find that large pretrained autoencoders (AEs) are surprisingly suitable for image steganography. Unlike previous methods, our model performs message reconstruction in the latent space of an AE, enabling effortless training of a pair of steganographic encoder-decoder using a simple L2 loss. With a small number of additional tunable parameters (around 10% of the base AE), our method reconstructs intricate high-frequency details. To further examine our method in practical scenarios, we test it in two novel applications: reversible visual censorship and reversible AI editing. Additionally, we provide modules to enhance our model's robustness to cropping, quantization, and compression. Our method is straightforward and easy to implement. Despite its simplicity, it works effectively on both in-the-wild images and standard datasets across various applications, offering competitive perceptual quality compared to specialized models.">
<meta property="og:description" content="As large pretrained models gain traction, we find that large pretrained autoencoders (AEs) are surprisingly suitable for image steganography. Unlike previous methods, our model performs message reconstruction in the latent space of an AE, enabling effortless training of a pair of steganographic encoder-decoder using a simple L2 loss. With a small number of additional tunable parameters (around 10% of the base AE), our method reconstructs intricate high-frequency details. To further examine our method in practical scenarios, we test it in two novel applications: reversible visual censorship and reversible AI editing. Additionally, we provide modules to enhance our model's robustness to cropping, quantization, and compression. Our method is straightforward and easy to implement. Despite its simplicity, it works effectively on both in-the-wild images and standard datasets across various applications, offering competitive perceptual quality compared to specialized models."><link rel="preload" as="fetch" crossorigin="anonymous" href="/_payload.json"><link rel="modulepreload" as="script" crossorigin href="/custom_assets/entry.02667d87.js"><link rel="preload" as="style" href="/custom_assets/entry.977e632c.css"><link rel="prefetch" as="script" crossorigin href="/custom_assets/error-404.4b915e71.js"><link rel="prefetch" as="script" crossorigin href="/custom_assets/error-500.24af93f8.js"><link rel="stylesheet" href="/custom_assets/entry.977e632c.css"><style>.header[data-v-1f19a5f7]{margin:0 auto;max-width:1200px;padding:2rem}.title[data-v-1f19a5f7]{font-size:2.1rem;margin-bottom:1rem}.title[data-v-1f19a5f7],.venue[data-v-1f19a5f7]{font-weight:500;text-align:center}.venue[data-v-1f19a5f7]{font-size:1.5rem}.affiliation-list[data-v-1f19a5f7],.author-list[data-v-1f19a5f7]{-moz-column-gap:1.5rem;column-gap:1.5rem;display:flex;flex-wrap:wrap;font-size:1.2rem;justify-content:center;text-align:center}.author-list>div[data-v-1f19a5f7]{white-space:nowrap}@media (max-width:850px){.title[data-v-1f19a5f7] div{color:var(--color-text-mute);font-size:1.2rem}}@media (min-width:850px){.title[data-v-1f19a5f7] div{display:inline;font-weight:inherit}.title[data-v-1f19a5f7] div:before{content:": ";font-weight:inherit}}.link-list[data-v-1f19a5f7]{display:flex;flex-wrap:wrap;font-size:1.2rem;gap:1rem 2rem;justify-content:center;margin-top:2rem}.link-list>a[data-v-1f19a5f7]:after{all:initial}.link-list>a>div[data-v-1f19a5f7]{background-color:var(--color-background-accent);border-radius:100px;color:var(--color-text-accent);padding:.2rem 1.3rem;transition:transform .2s}@media (hover:hover){.link-list>a>div[data-v-1f19a5f7]:hover{transform:scale(1.05)}}.link-list>a>div[data-v-1f19a5f7]:active{transform:scale(1.05)}@media (prefers-color-scheme:dark){.link-list>a>div[data-v-1f19a5f7]{border:1px solid #fff}}i[data-v-1f19a5f7]{margin-right:.1rem}</style><style>.medium-zoom-image--opened[data-v-3e6d5202],.medium-zoom-overlay[data-v-3e6d5202]{border:initial;border-radius:0;z-index:999}img[data-v-3e6d5202]{border-radius:5px;width:100%}@media (prefers-color-scheme:light){img[data-v-3e6d5202]{border:.1rem solid #ddd}}</style><style>svg[data-v-c136a475]{width:1rem}</style><style>.main[data-v-1801dce1]{margin:0 auto;max-width:1200px;padding:2rem}.section-title[data-v-1801dce1]{font-size:2rem;font-weight:500;margin:2rem 0 .5rem;text-align:center}.caption[data-v-1801dce1]{font-size:1rem;line-height:1.6;margin-top:.2rem;padding:0 2.5rem;text-align:center}.image[data-v-1801dce1]{line-height:0;margin:1.5rem 0 1rem}@media (max-width:650px){.caption[data-v-1801dce1]{font-size:1rem;padding:0}}p[data-v-1801dce1]{margin:.5rem 0 1rem}</style><style>.footer[data-v-5dc915b3]:before{background-image:url(/images/backgrounds/wave.png);background-position:bottom;content:" ";height:188px;pointer-events:none;position:absolute;top:-187px;width:100%}.footer[data-v-5dc915b3],.footer[data-v-5dc915b3]:before{background-repeat:no-repeat;background-size:cover}.footer[data-v-5dc915b3]{background-color:var(--color-background-accent);background-image:url(/images/backgrounds/footer.jpg);background-position:top;margin-top:100px;max-width:100%;min-height:300px;position:relative;width:calc(100vw - var(--scrollbarWidth))}.footer[data-v-5dc915b3],.footer a[data-v-5dc915b3]{color:var(--color-text-accent)}.content[data-v-5dc915b3]{margin:0 auto;max-width:900px;padding:2rem;position:relative;z-index:1}.logo[data-v-5dc915b3]{display:flex;padding-bottom:calc(var(--section-gap)/4);place-items:flex-start}img[data-v-5dc915b3]{width:200px}@media (min-width:900px){.footer[data-v-5dc915b3]{display:flex;place-items:center}.content[data-v-5dc915b3]{display:grid;grid-template-columns:1fr 3fr;padding:0 2rem}.logo[data-v-5dc915b3]{padding-bottom:0;padding-right:calc(var(--section-gap)/2)}}</style><style>@import url("https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css");@import url("https://site-assets.fontawesome.com/releases/v6.4.2/css/all.css");:root{--vt-c-white:#fff;--vt-c-white-soft:#f8f8f8;--vt-c-white-mute:#f2f2f2;--vt-c-black:#181818;--vt-c-black-soft:#222;--vt-c-black-mute:#282828;--vt-c-indigo:#121314;--vt-c-divider-light-1:rgba(60,60,60,.29);--vt-c-divider-light-2:rgba(60,60,60,.12);--vt-c-divider-dark-1:rgba(84,84,84,.65);--vt-c-divider-dark-2:rgba(84,84,84,.48);--vt-c-text-light-1:var(--vt-c-indigo);--vt-c-text-light-2:rgba(0,0,0,.66);--vt-c-text-dark-1:var(--vt-c-white);--vt-c-text-dark-2:hsla(0,0%,92%,.64);--vt-c-asblue:#153146;--color-background:var(--vt-c-white);--color-background-soft:var(--vt-c-white-soft);--color-background-mute:var(--vt-c-white-mute);--color-background-accent:var(--vt-c-asblue);--color-border:var(--vt-c-divider-light-2);--color-border-hover:var(--vt-c-divider-light-1);--color-heading:var(--vt-c-text-light-1);--color-text:var(--vt-c-text-light-1);--color-text-mute:var(--vt-c-text-light-2);--color-text-accent:var(--vt-c-text-dark-1);--section-gap:160px}@media (prefers-color-scheme:dark){:root{--color-background:var(--vt-c-black);--color-background-soft:var(--vt-c-black-soft);--color-background-mute:var(--vt-c-black-mute);--color-background-accent:var(--vt-c-asblue);--color-border:var(--vt-c-divider-dark-2);--color-border-hover:var(--vt-c-divider-dark-1);--color-heading:var(--vt-c-text-dark-1);--color-text:var(--vt-c-text-dark-1);--color-text-mute:var(--vt-c-text-dark-2);--color-text-accent:var(--vt-c-text-dark-1)}}*,:after,:before{box-sizing:border-box;font-weight:400;margin:0}body{-webkit-font-smoothing:antialiased;-webkit-text-size-adjust:100%;-moz-osx-font-smoothing:grayscale;background:var(--color-background);color:var(--color-text);font-family:Inter,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen,Ubuntu,Cantarell,Fira Sans,Droid Sans,Helvetica Neue,sans-serif;font-size:1em;line-height:1.6;text-rendering:optimizeLegibility;transition:color .5s,background-color .5s}:root{font-size:20px}@media (max-width:850px){:root{font-size:16px}}@media (max-width:450px){:root{font-size:12px}}#app{font-weight:400;margin:0 auto;max-width:1280px;padding:2rem}a{color:var(--color-text);cursor:pointer;position:relative;text-decoration:none;white-space:nowrap}a:after{background-color:rgba(179,153,59,.5);bottom:0;content:"";left:-.1em;position:absolute;right:-.1em;top:66%;transition:top .2s cubic-bezier(0,.8,.13,1);z-index:-1}@media (hover:hover){a:hover:after{top:0}}a:active:after{top:0}@media (min-width:1024px){body{display:flex;place-items:center}#app{display:grid;grid-template-columns:1fr 1fr;padding:0 2rem}}</style></head>
<body ><div id="__nuxt"><!--[--><div class="header" data-v-1f19a5f7><div class="title" data-v-1f19a5f7>ReVP<div>Reversible Visual Processing with Latent Models</div></div><div class="venue" data-v-1f19a5f7>2024</div><div class="author-list" data-v-1f19a5f7><!--[--><div data-v-1f19a5f7><!--[-->Anonymous<!--]--><!----></div><!--]--></div><div class="affiliation-list" data-v-1f19a5f7><!--[--><div data-v-1f19a5f7><!----></div><!--]--></div><div class="link-list" data-v-1f19a5f7><!--[--><a href="https://youtu.be/v0lHsiwnxh0" target="_blank" data-v-1f19a5f7><div data-v-1f19a5f7><i class="fa-brands fa-youtube" data-v-1f19a5f7></i> Video</div></a><!--]--></div></div><div class="main" data-v-1801dce1><div class="image" data-v-1801dce1><img src="/images/teaser.jpg" data-v-1801dce1 data-v-3e6d5202><div class="caption" data-v-1801dce1>Our results on images in-the-wild. Our method, ReVP, can hide an image inside another image. Consider various irreversible image processing operations, such as censorship and AI editing, we hide each original image in its processed version. The figure shows our reconstructions and the processed images.</div></div><div class="section-title" data-v-1801dce1>Abstract</div><p data-v-1801dce1> As large pretrained models gain traction, we find that large pretrained autoencoders (AEs) are surprisingly suitable for image steganography. Unlike previous methods, our model performs message reconstruction in the latent space of an AE, enabling effortless training of a pair of steganographic encoder-decoder using a simple L2 loss. With a small number of additional tunable parameters (around 10% of the base AE), our method reconstructs intricate high-frequency details. To further examine our method in practical scenarios, we test it in two novel applications: reversible visual censorship and reversible AI editing. Additionally, we provide modules to enhance our model&#39;s robustness to cropping, quantization, and compression. Our method is straightforward and easy to implement. Despite its simplicity, it works effectively on both in-the-wild images and standard datasets across various applications, offering competitive perceptual quality compared to specialized models. </p><div class="section-title" data-v-1801dce1>Architecture</div><div class="image" data-v-1801dce1><img src="/images/system-diagram.jpg" data-v-1801dce1 data-v-3e6d5202><div class="caption" data-v-1801dce1>MeDM mediates independent image score estimations after every denoising step. Inspired by the fact that video pixels are essentially views to the underlying objects, we construct an explicit pixel repository <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewbox="0 0 10 8" version="1.1" data-v-1801dce1 data-v-c136a475><defs data-v-c136a475><g data-v-c136a475><symbol overflow="visible" id="glyph0-0" data-v-c136a475><path style="stroke:none;" d="M 4.03125 -6.265625 C 5.953125 -6.265625 6.453125 -5.796875 6.453125 -5.140625 C 6.453125 -4.53125 5.96875 -3.328125 4.328125 -3.265625 C 3.84375 -3.25 3.5 -2.90625 3.5 -2.796875 C 3.5 -2.734375 3.53125 -2.734375 3.546875 -2.734375 C 3.96875 -2.65625 4.171875 -2.46875 4.6875 -1.28125 C 5.140625 -0.234375 5.40625 0.21875 5.984375 0.21875 C 7.203125 0.21875 8.34375 -1 8.34375 -1.21875 C 8.34375 -1.296875 8.265625 -1.296875 8.234375 -1.296875 C 8.109375 -1.296875 7.71875 -1.15625 7.515625 -0.875 C 7.359375 -0.640625 7.140625 -0.328125 6.65625 -0.328125 C 6.140625 -0.328125 5.828125 -1.03125 5.5 -1.8125 C 5.28125 -2.3125 5.109375 -2.6875 4.875 -2.953125 C 6.3125 -3.484375 7.296875 -4.53125 7.296875 -5.5625 C 7.296875 -6.8125 5.625 -6.8125 4.109375 -6.8125 C 3.125 -6.8125 2.5625 -6.8125 1.71875 -6.453125 C 0.390625 -5.859375 0.203125 -5.03125 0.203125 -4.953125 C 0.203125 -4.890625 0.25 -4.875 0.3125 -4.875 C 0.46875 -4.875 0.703125 -5.015625 0.78125 -5.0625 C 0.984375 -5.203125 1.015625 -5.265625 1.078125 -5.453125 C 1.21875 -5.859375 1.5 -6.203125 2.75 -6.265625 C 2.703125 -5.65625 2.609375 -4.71875 2.265625 -3.265625 C 2 -2.15625 1.640625 -1.0625 1.203125 0.015625 C 1.140625 0.125 1.140625 0.140625 1.140625 0.15625 C 1.140625 0.21875 1.21875 0.21875 1.25 0.21875 C 1.453125 0.21875 1.859375 -0.015625 1.96875 -0.203125 C 2 -0.265625 3.265625 -3.078125 3.5625 -6.265625 Z M 4.03125 -6.265625 " data-v-c136a475></path></symbol></g></defs><g id="surface1" data-v-c136a475><g style="fill:currentColor;fill-opacity:1;" data-v-c136a475><use xlink:href="#glyph0-0" x="0.716016" y="7.766016" data-v-c136a475></use></g></g></svg> to represent the underlying world. For more details, please refer to our <a href="/medm.pdf" target="_blank" data-v-1801dce1>paper</a>. </div></div><div class="section-title" data-v-1801dce1>Results</div><div class="image" data-v-1801dce1><img src="/images/full-teaser.jpg" data-v-1801dce1 data-v-3e6d5202><div class="caption" data-v-1801dce1>Full side-by-side comparisons</div></div></div><div class="footer" data-v-5dc915b3><div class="content" data-v-5dc915b3><div class="logo" data-v-5dc915b3></div><div data-v-5dc915b3> This website is licensed under a Creative Commons Attribution-ShareAlike 4.0 International <a href="https://creativecommons.org/licenses/by-sa/4.0/" target="_blank" data-v-5dc915b3>License</a>. <br data-v-5dc915b3><br data-v-5dc915b3> This means you are free to borrow the <a href="https://github.com/medm2023/medm2023.github.io" target="_blank" data-v-5dc915b3>source code</a> of this website, we just ask that you link back to this page in the footer. </div></div></div><!--]--></div><script type="application/json" id="__NUXT_DATA__" data-ssr="true" data-src="/_payload.json">[{"state":1,"_errors":3,"serverRendered":5,"path":6,"prerenderedAt":7},["Reactive",2],{},["Reactive",4],{},true,"/",1716877437605]</script><script>window.__NUXT__={};window.__NUXT__.config={public:{},app:{baseURL:"/",buildAssetsDir:"custom_assets",cdnURL:""}}</script><script type="module" src="/custom_assets/entry.02667d87.js" crossorigin></script></body>
</html>